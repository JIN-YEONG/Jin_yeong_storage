 keras 1~4 까지의 키워드

 Sequentail - 순차형 모델 생성

 Dense -keras레이어의 한 종류로 일반적인 2D데이터를 학습하는데 사용된다.
        기본 적으로 Dense(x)로 사용되며 x값으로 출력 노드의 개수를 정할 수 있다. 

 input_dim = 입력 데이터의 차원을 결정한다. 
             input_shape를 사용해 고차원 데이터를 입력받을 수도 있다.

 activation   활성화 함수를 설정한다 
              활성화 함수란 입력을 받아 활성, 비활성을 결정하는데 사용하는 함수

 relu - 활성화 함수의 한 종류로 계산값이 0이상이면 그대로 출력하고 0이하면 0으로 출력한다.
        대부분의 경우에 대부분의 경우에 사용하는 활성화 함수이다.

 loss- 손실 함수: 훈련데이터에서 신경망의 성능을 측정하는 방법으로 
        신경망이 옳바른 방향으로 학습될 수 있도록 도와준다.

 mse - mean squared error 손실 함수의 한 종류로 평균제곱오차를 나타낸다. 
        실제 관측값과 모델이 예측한 값의 차이를 제곱한후 모두 더해 평균을 구한다.
        실제 관측값과 예측값의 차이가 음수인 경우를 피하기 위해 제곱한다.
        주로 선형회귀 모델의 비용 함수로 사용한다.

 optimizer - 옵티마이저: 입력된 데이터와 소실함수를 기반으로 네트워크를 최적의 방향으로 
            업데이트 하는 메커니즘

 adam - optimizer의 한 종류로 가장 보편적으로 사용되고 결과 값도 좋다.


optimizer의 종류
1. 확률적 경사하강법(Stochastic Gradient Descent)
   손실함수의 기울기를 계산하여서 이 기울기 값에 대한 학습률을 계산하여
   이 결과 값으로 기존의 가중치 값을 갱신한다.

   경사 하강법은 무작정 기울어진 방향으로 이동하는 방식이기 때문에 비효율적이여 
   탐색 시간이 길다. 또한 적합하지 않아도 최소값의 방향으로만 움직이기 때문에 
   최저점으로 이동하지 않는 경우가 있어 비효율적이다.

2. 모먼트(Momentum)
    기울기에서 속도의 개념이 추가된 것으로 속도가 크게 나올수록 기울기가
    크게 업데이트 되어 확룰적 경사하강법의 단점을 보완할 수 있다.

3. adagrad
    신경망 학습에 학습률(learning_rate)를 주어 한번에 갱신하는 가중치의 값을 결정
    과거의 기울기 값을 제곱해서 계속 더하는 식으로 학습률을 낮춘다.
    학습률이 0에 가까워져서 진행이 안되는 문제가 발생한다.

4. RMSProp
    adagrad의 문제점을 개선 과거의 모든 기울기를 균일하게 더하지 않고 새로운 기울기의 정보만 
    반영하여 학습률이 0에 가까워지는 것을 방지함

5. adam
    가장 많이 사용하는 방법 잘모르겠으면 일단 사용해 본다.
    Momentum와 adagrad을 섞은 기법


 metrics - 훈련과 테스트 과정을 모니터링할 지표를 설정해준다.

 accuracy  - 훈련 결과의 정확도를 출력 백분률로 출력되며 주로 분류 모델에 많이 사용된다.
 epochs  - 훈련 횏수
 batchsize - 한번에 네트워크에 넘겨줄 데이터의 수 값이 끌수록 한번에 학습하는 데이터가 많기 때문에
                학습 속도가 빨라지지만 최적의 결과를 얻디 위해서는 적당한 크기의 batchsize가 좋으며
                input_data를 나룰수 있는 값이 이상적이다. 기본적으로는 작은 batchsize가 좋다.
 compile - 모델의 모든 레이어, loss, optimizer등을 컴파일한다.
 fit   - 컴파일된 모델에 input_data를 넣어 훈련을 시작한다. 여기서 훈련의 횟수는 epochs값으로 결정
 evaluate  훈련된 모델에 다른 데이터를 넣어서 평가한다. loss값과 정확도값(acc,mse)이 나온다.
 summary  - 모델의 구조를 출력한다. 각 레이어의 파라미터 개수, input, output 노드의 개수 등이 출력

파리미터 계산법
1. DNN
    이전 레이어의 output개수 + 1(bias의 개수)를 한 값에 해당 레이어의 output노드의 개수를 곱한다.
    (n(input_node) + 1) * n(output_node)

2. CNN
    필터의 shape를 각각 곱하고 입력 값(입력층에서는 캐널의 값)을 곱하고 
    출력값을 곱한후 출력값 만큼의 bias값을 더한다.
    출력 값만큼의 채널이 생긴가는 뜻이기 때문에 bias도 채널 수 만큼 생신다.
    (kernal_size[0] * kernal_size[1]) * (n(input_node)) * (n(output_node)) + n(ouput_node)

3. RNN(LSTM)
    시계열 데이터를 다루고 있기 때문에 현재의 가중치 값이 이전 가중치 값의 영향을 받아 결정된다.
    때문에 매우 복잡한 파라미터를 가지고 있다. (구글에서 계산 법을 찾기 힘들다.)

 predict  - 훈련된 모델에 새로운 데이터를 넣어 그 데이터에 해당하는 결과 값(예측, 분류)을 산출한다.
